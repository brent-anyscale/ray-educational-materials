{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb9a269-765a-46ef-abb6-d60682d38b0d",
   "metadata": {},
   "source": [
    "# Scalable Batch Inference with Ray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b114120-a325-4597-bf7b-8310c9c8d776",
   "metadata": {},
   "source": [
    "<img src=\"../../_static/assets/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "### Is this module right for you?\n",
    "\n",
    "This module presents several approaches for scaling batch inference on Ray. Through hands-on practice with inference on a computer vision task, you will implement and compare different inference architectures to better understand Ray Core and Ray AIR.\n",
    "\n",
    "To get the most out of this notebook, the following scenarios may apply to you:\n",
    "\n",
    "* You observe performance bottlenecks when working on batch inference problems in computer vision projects.\n",
    "* You want to scale or increase throughput of existing batch inference pipelines.\n",
    "* You wish to explore different architectures for scaling batch inference with Ray Core and Ray AIR.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "For this notebook you should satisfy the following requirements:\n",
    "\n",
    "* Practical Python and machine learning experience.\n",
    "* Familiarity with batch inference in ML.\n",
    "* Familiarity with Ray and Ray AIR equivalent to completing these training modules:\n",
    "  * [Overview of Ray](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Overview_of_Ray.ipynb)\n",
    "  * [Introduction to Ray AIR](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Introduction_to_Ray_AIR.ipynb)\n",
    "  * [Ray Core](https://github.com/ray-project/ray-educational-materials/tree/main/Ray_Core)\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "* Understand common design patterns for distributed batch inference.\n",
    "* Implement scalable batch inference with Ray.\n",
    "* Extend each approach by tuning performance.\n",
    "* Compare scalable batch inference architectures on Ray to evaluate which is most relevant to your work.\n",
    "\n",
    "### What will you do?\n",
    "\n",
    "* Learn about three distributed batch inference design patterns with Ray.\n",
    "* Get to know the inference task.\n",
    "  * Semantic (image) segmentation using the SegFormer model.\n",
    "* Implement sequential inference.\n",
    "* Implement distributed inference patterns.\n",
    "  1. Stateless inference with Ray Tasks\n",
    "  2. Stateful inference with Ray Actors and ActorPool utility\n",
    "  3. Inference with Ray AIR Datasets and BatchPredictor abstractions.\n",
    "* Compare approaches to identify situations best fit for each."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35a02709-0ed5-489a-b344-27005261eb72",
   "metadata": {},
   "source": [
    "## Part 1: Ray design patterns for scaling batch inference\n",
    "\n",
    "The ultimate goal for machine learning models is often to generate predictions on a set of unseen data. In this notebook, you focus on the inference stage of the ML workflow and explore different approaches to scaling it.\n",
    "\n",
    "Ray Core and Ray AIR provide APIs that allow you to perform batch inference at scale, processing millions of examples and offering various performance tuning options.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/example_ml_workflow.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|An example of a machine learning workflow that starts with reading raw data and preprocessing it. These steps are followed by training and tuning that produce a trained model. This model is then used for inference, often on large datasets.|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2450181f",
   "metadata": {},
   "source": [
    "### What is (batch) inference?\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>Batch inference</strong> (also known as offline inference): is the process of generating predictions on a large set or \"batch\" of data.\n",
    "</div>\n",
    "\n",
    "Unlike *online inference* where predictions are generated as each observation is produced, batch inference generates predictions over a large number of input data when immediate response is not required or feasible. \n",
    "\n",
    "For example, batch inference is relevant when generating weekly product recommendations using historical customer data or sales forecasting using time-aggregated observations.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/batch_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Batch inference is the process of applying a trained model to a batch of data to generate predictions.|\n",
    "\n",
    "In a non-distributed setting, inference executes sequentially. The model processes incoming batches of data one at a time, limiting performance to a single machine or GPU. Below, you will learn about three approaches for distributing batch inference on Ray."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8becdd0-3f1e-4503-92b6-0e275e03ad75",
   "metadata": {},
   "source": [
    "### Stateless inference using Ray Tasks\n",
    "\n",
    "Part of the Ray Core primitives, [Ray Tasks](https://docs.ray.io/en/latest/ray-core/tasks.html#ray-remote-functions) offer an easy way to distribute inference across a compute cluster. Tasks are Python functions that execute remotely in the cluster, allowing multiple processes to work on different tasks concurrently (see: [Remote Procedure Call](https://en.wikipedia.org/wiki/Remote_procedure_call)).\n",
    "\n",
    "In this approach, tasks contain replicas of the trained model to compute predictions on input data. Since tasks do not store or modify any internal state, we say they are *stateless*. \n",
    "\n",
    "An example of a stateless function in deep learning is the [SGD optimizer](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) because it only updates weights (the output) based on the gradient of the loss function (the input, along with the current weights). No internal state about previously calculated gradients influences how future gradients are calculated.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/task_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|During stateless inference, each Ray Task loads the trained model and outputs predictions on assigned batches. This approach scales with the number of available CPUs and GPUs because each inference task is independent of the other concurrent jobs.|\n",
    "\n",
    "<img src=\"../../_static/assets/Scaling_inference/code_task.png\" width=\"70%\" loading=\"lazy\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5687ea84-b693-4a97-8de4-26954a4c97c4",
   "metadata": {},
   "source": [
    "### Stateful inference using Ray Actors\n",
    "\n",
    "In the previous approach, the trained model is loaded and discarded after each batch. This works great for smaller models, however, loading large, complex models into memory can be computationally expensive. In addition, you may want the ability to capture some persistent internal state.\n",
    "\n",
    "[Ray Actors](https://docs.ray.io/en/latest/ray-core/actors.html) are *stateful objects*, meaning they maintain an internal state. Other examples of stateful objects include Python classes and the [Adam optimizer](https://arxiv.org/abs/1412.6980) commonly used in deep learning. Due to this property, actors can run inference on multiple batches and avoid the overhead of reloading the model after each batch.\n",
    "\n",
    "Setting up stateful inference involves a few important steps:\n",
    "\n",
    "1. Create replicas of the trained model as Ray Actors.\n",
    "2. Feed data into these model replicas in parallel and retrieve predictions.\n",
    "3. Continue to manage idle actors and assign tasks until the entire inference job completes.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/actor_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray Actors can generate predictions on batches of data. Because each actor keeps track of an internal state, it can be reused for inference on multiple batches.|\n",
    "\n",
    "<img src=\"../../_static/assets/Scaling_inference/code_actor.png\" width=\"70%\" loading=\"lazy\">\n",
    "\n",
    "When using Ray Actors for stateful inference, it is important to implement *load balancing*, or appropriate distribution of work among workers to utilize resources efficiently. This process involves keeping track of in-flight tasks to assign new batches to available actors continuously until the entire process completes.\n",
    "\n",
    "Using actors directly offers more control over how tasks are assigned. However, you may opt to use the convenient [Ray ActorPool](https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-util-actorpool) utility which handles load balancing (futures management) automatically. This abstraction wraps a list of actors and distributes the workload, allowing you to focus on the inference logic.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/actor_pool.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|The ActorPool wraps around a list of `n` actors so you do not have to manage idle actors and manually distribute workloads.|\n",
    "\n",
    "<img src=\"../../_static/assets/Scaling_inference/code_actorpool.png\" width=\"70%\" loading=\"lazy\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c500d89-bdae-41fd-9b12-264c0839af90",
   "metadata": {},
   "source": [
    "### Batch inference using Ray AIR BatchPredictor\n",
    "\n",
    "In the previous approaches with tasks and actors, there remain some tricky aspects to discuss:\n",
    "* Small batches may be inefficient when dispatched sequentially.\n",
    "* Large batches may cause OutOfMemory errors (e.g. cannot fit on GPU).\n",
    "* Sending one task at a time to actors potentially limits performance.\n",
    "* Data fetching and batch processing could be parallelized.\n",
    "\n",
    "While each of these could be addressed by implementing performance optimizations using Ray Core primitives, [Ray AIR](https://docs.ray.io/en/latest/ray-air/getting-started.html) offers high-level APIs that have these optimizations built-in.\n",
    "\n",
    "Ray AIR [BatchPredictor](https://docs.ray.io/en/latest/ray-air/predictors.html#batch-prediction) is a utility for large-scale, distributed batch inference. It takes in two components:\n",
    "\n",
    "* **`Checkpoint`**. A trained model, could be from training or tuning step.\n",
    "* **`Predictor`**. A class that loads models from `Checkpoint` to perform inference; supports framework-specific predictors (e.g. TorchPredictor and TensorflowPredictor).\n",
    "\n",
    "Once instantiated, BatchPredictor can call `predict()` on a Ray Dataset. [Ray Datasets](https://docs.ray.io/en/latest/data/dataset.html#datasets) are the standard way to load and exchange data in Ray AIR. Datasets load and preprocess data for parallel compute, internally handling operations like batching, pipelining, autoscaling the actor pool, and memory management.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/air_batchpredictor.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray Datasets parallelize data loading, preprocessing, and batching. Ray AIR `BatchPredictor` takes both `Checkpoint` and `Predictor` objects to call `predict()` on a Ray Dataset for distributed batch inference.|\n",
    "\n",
    "These high-level abstractions automate the challenging aspects of scaling batch inference in exchange for less direct control over the way Ray distributes.\n",
    "\n",
    "<img src=\"../../_static/assets/Scaling_inference/code_batchpredictor.png\" width=\"70%\" loading=\"lazy\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ee761b0-7549-46ca-99bc-e3588a7a1deb",
   "metadata": {},
   "source": [
    "## Part 2: Batch inference example using computer vision transformers\n",
    "\n",
    "To demonstrate the three design patterns introduced in the previous section, you will apply each approach on a computer vision task: semantic segmentation.\n",
    "\n",
    "Semantic segmentation, similar to object detection, involves assigning labels to objects in a scene pixel-by-pixel. In this hands-on example, you will run batch inference on image data by using a pretrained model to generate predictions.\n",
    "\n",
    "### Data\n",
    "\n",
    "#### MIT ADE20K - scene parsing benchmark\n",
    "\n",
    "The [MIT ADE20K Dataset](http://sceneparsing.csail.mit.edu/) (also known as \"SceneParse150\") provides the largest open source dataset for scene parsing. It is often used as a standard for assessing semantic segmentation model performance due to its high-quality annontations. For this example, you will use the unlabeled test data to implement different batch inference architectures.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/scene.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Unannotated scene image from MITADE20K on the left. Pixel-by-pixel predictions on the right. [*Date accessed: November 10, 2022*](https://github.com/CSAILVision/semantic-segmentation-pytorch)|\n",
    "\n",
    "Dataset highlights\n",
    "\n",
    "* 20k annotated, scene-centric training images\n",
    "* 3.3k unlabeled test images\n",
    "* 150 [semantic categories](https://docs.google.com/spreadsheets/d/1se8YEtb2detS7OuPE86fXGyD269pMycAWe2mtKUj2W8/edit?usp=sharing) (such as person, car, bed, sky, etc.)\n",
    "\n",
    "### Model\n",
    "\n",
    "#### SegFormer - transformer-based framework for semantic segmentation\n",
    "\n",
    "[SegFormer](https://arxiv.org/pdf/2105.15203.pdf) is an effective semantic segmentation method based on a *transformer* architecture. [Transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) are a type of deep learning architecture that process sequential data via a series of self-attention layers and then transform them via a feedforward neural network.\n",
    "\n",
    "What sets SegFormer apart from previous transformer-based approaches are two key features:\n",
    "\n",
    "1. A hierarchically structured transformer encoder which does not depend on positional encoding that avoids interpolation when training and testing resolutions differ.\n",
    "2. A lightweight MLP layer that avoids complex decoders.\n",
    "\n",
    "You will use a pretrained SegFormer model finetuned on [MITADE20K](http://sceneparsing.csail.mit.edu/) to perform batch inference.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/segformer_architecture.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|SegFormer architecture showcasing the hierarical transformer encoder and all-MLP decoder. [*Date accessed: November 10, 2022*](https://arxiv.org/pdf/2105.15203.pdf).|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018d564-0e4a-453f-9f7a-74466c0273e8",
   "metadata": {},
   "source": [
    "## Part 3: Sequential batch inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a07b49f-ee7b-48c7-a4d0-bc4043ff24c9",
   "metadata": {},
   "source": [
    "In order to establish familiarity with this batch inference task, you will implement a basic approach with one worker that generates predictions on batches sequentially. To get set up, the semantic segmentation example requires the following steps:\n",
    "\n",
    "1. Load the pretrained [SegFormer](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512) model.\n",
    "2. Load the [feature extractor](https://huggingface.co/docs/transformers/v4.16.2/en/model_doc/segformer#transformers.SegformerFeatureExtractor) (preprocessor for scene data).\n",
    "3. Load [SceneParse150](https://huggingface.co/datasets/scene_parse_150) dataset.\n",
    "4. Run batch inference on images from the test set.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/single_sequential_timeline.png\" width=\"90%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Timeline of sequential batch inference using a single worker. Tasks can vary in runtime due variations in complexity, data size, and more. |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97f83049",
   "metadata": {},
   "source": [
    "### Set up necessary imports and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb5419c-26af-4b09-8fd2-e885aaa60021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from PIL.JpegImagePlugin import JpegImageFile\n",
    "\n",
    "# Set the seed to a fixed value for reproducibility.\n",
    "torch.manual_seed(201)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "245bf0ab",
   "metadata": {},
   "source": [
    "### Load the model components from the HuggingFace Hub\n",
    "\n",
    "From the [Hugging Face Hub](https://huggingface.co/docs/hub/index), retrieve the pretrained SegFormer model by specifying the model name and [label files](https://huggingface.co/datasets/huggingface/label-files/blob/main/ade20k-id2label.json) which map indices to semantic categories."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ff44bdc",
   "metadata": {},
   "source": [
    "#### Load label mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32feab-170b-4220-a263-bb678592d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4346c5a0-36b4-46ce-bdc0-3102543d2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label, label2id = get_labels()\n",
    "\n",
    "print(f\"Total number of labels: {len(id2label)}\")\n",
    "print(f\"Example labels: {list(id2label.values())[:5]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9354716-8ac2-4e4f-97e1-e48ae174f0d9",
   "metadata": {},
   "source": [
    "The utility function `get_labels` fetches two dictionary mappings from [Hugging Face](https://huggingface.co/datasets/huggingface/label-files/blob/main/ade20k-id2label.json), `id2label` and `label2id`, which are used to convert between numerical and string labels for the 150 available [semantic categories](https://docs.google.com/spreadsheets/d/1se8YEtb2detS7OuPE86fXGyD269pMycAWe2mtKUj2W8/edit#gid=0) of objects."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d57314e",
   "metadata": {},
   "source": [
    "#### Load SegFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f3990-3c6f-4ae4-9186-fa9fb1800f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerForSemanticSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aec6f8-22bf-41f9-bd7b-fe54889d762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    "\n",
    "segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    MODEL_NAME, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"Number of model parameters: {segformer.num_parameters()/(10**6):.2f} M\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24f3ef57",
   "metadata": {},
   "source": [
    "The [Hugging Face Hub](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512) makes available many variations on SegFormer. Here, you specify a version finetuned on the MITADE20K (SceneParse150) dataset on images with a 512 x 512 resolution.\n",
    "\n",
    "Note: This \"b0\" model is the smallest, with [other options](https://huggingface.co/nvidia/segformer-b5-finetuned-ade-640-640) ranging up to and including \"b5\". Keep this in mind as something to experiment with when comparing different batch inference architectures later on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41546abd-2f3d-4128-8176-19cd0604d6f4",
   "metadata": {},
   "source": [
    "#### Load the feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51be71f-1c3f-45fb-b9b2-af3393bfadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888ed08-cde2-4f4a-b05f-e467de4b7ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "segformer_feature_extractor = SegformerFeatureExtractor.from_pretrained(\n",
    "    MODEL_NAME, reduce_labels=True\n",
    ")\n",
    "segformer_feature_extractor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a9a23f3",
   "metadata": {},
   "source": [
    "[Feature extractors](https://huggingface.co/docs/transformers/main_classes/feature_extractor) preprocess input features (e.g. image data) by normalizing, resizing, padding, and converting raw images into the shape expected by SegFormer.\n",
    "\n",
    "The [`reduce_labels`](https://huggingface.co/docs/transformers/model_doc/segformer#segformer) flag ensures that the background of an image (anything that is not explicitly an object) isn't included when computing loss. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3109152b-3572-40f9-8fc6-b04ecce59896",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3d58d4b-d13c-4c34-8e80-194c8f6bdae9",
   "metadata": {},
   "source": [
    "#### Set up necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e93db0-6218-4460-8f8c-14703a57f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from utils import convert_image_to_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24ed45d-8700-40de-99ac-2bd4c5999974",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_DATA = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b25fc198-d3f4-47a0-9c16-9054a342c4f7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>SMALL_DATA</strong>: a flag to download a subset (160 images) of the available test data. Defaults to True. Set to False (recommended) to work with the full test data (3352 images).\n",
    "</div>\n",
    "\n",
    "If you set `SMALL_DATA` to `False`, expect it to take some time (depending on your connection download speed) because you are downloading all test images to your local machine or cluster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3066409",
   "metadata": {},
   "source": [
    "#### Load SceneParse150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f31fe-e8a9-41ea-857c-e2e2b36503e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"scene_parse_150\"\n",
    "\n",
    "# Load data from the Hugging Face datasets repository.\n",
    "if SMALL_DATA:\n",
    "    train_dataset = load_dataset(DATASET_NAME, split=\"train[:10]\")\n",
    "    test_dataset = load_dataset(DATASET_NAME, split=\"test[:160]\")\n",
    "else:\n",
    "    train_dataset = load_dataset(DATASET_NAME, split=\"train[:10]\")\n",
    "    test_dataset = load_dataset(DATASET_NAME, split=\"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98a16ce0",
   "metadata": {},
   "source": [
    "The two datasets serve different purposes:\n",
    "\n",
    "* **`train_dataset`**  \n",
    "    * Retrieve a small sample of images for visualization purposes only. Training samples include ground-truth, annotated image regions. Full training dataset contains 20210 images.\n",
    "* **`test_dataset`**  \n",
    "    * Used for batch inference purposes. Test samples do not contain ground-truth labels. Full test dataset contains 3352 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf19f5d-52cf-4d15-a958-e143bcf711c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadba2c1-eb5e-42f9-8de7-bb3311a04786",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.map(convert_image_to_rgb)\n",
    "test_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c6e66d8-2587-43c9-91bb-cdfc6f22b33d",
   "metadata": {},
   "source": [
    "Each sample contains three components:\n",
    "* **`image`** \n",
    "    * The PIL image.\n",
    "* **`annotation`**  \n",
    "    * Human annotations of image regions (annotation mask is `None` in testing set).\n",
    "* **`category`**  \n",
    "    * Category of the scene generally (e.g. driveway, voting booth, dairy_outdoor)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2a63ad-3bfd-453c-827e-6a0f664ac310",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Display example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c22f28-f63c-42e8-ab43-1684aa692d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_example_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1054dfa-90a6-4569-bc0f-cb059f4b4893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try running this multiple times!\n",
    "display_example_images(train_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf7243ec-d207-4b33-89ca-b80309c19c06",
   "metadata": {},
   "source": [
    "### Run sequential inference on 1 batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7eccd1dc",
   "metadata": {},
   "source": [
    "#### Define inference logic\n",
    "\n",
    "This `predict` function forms the basis for the inference step, and you will reuse variations of this function multiple times throughout each approach for batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba82c68-f3cd-46ae-8b23-6fee43ccb3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model: SegformerForSemanticSegmentation,\n",
    "    feature_extractor: SegformerFeatureExtractor,\n",
    "    images: list[JpegImageFile],\n",
    ") -> list[np.array]:\n",
    "\n",
    "    # Set the device on which PyTorch will run.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # Move the model to specified device.\n",
    "    model.eval()  # Set the model in evaluation mode on test data.\n",
    "\n",
    "    # The feature extractor processes raw images.\n",
    "    inputs = feature_extractor(images=images, return_tensors=\"pt\")\n",
    "\n",
    "    # The model is applied to input images in the inference step.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=inputs.pixel_values.to(device))\n",
    "\n",
    "    # Post-process the output for display.\n",
    "    image_sizes = [image.size[::-1] for image in images]\n",
    "    segmentation_maps_postprocessed = (\n",
    "        feature_extractor.post_process_semantic_segmentation(\n",
    "            outputs=outputs, target_sizes=image_sizes\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Return list of segmentation maps detached from the computation graph.\n",
    "    return [j.detach().cpu().numpy() for j in segmentation_maps_postprocessed]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "336d6a94-8399-4616-8221-e1f5c357baf2",
   "metadata": {},
   "source": [
    "#### Prepare 1 batch of 16 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70115952-fdb0-40d3-87d0-a7903502a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_image_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f4caf-8964-4170-8bde-b126a5a0ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "# Get BATCH_SIZE randomly shuffled image IDs from the test dataset.\n",
    "image_indices = get_image_indices(dataset=test_dataset, n=BATCH_SIZE)\n",
    "image_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3302db4-6b93-4ef6-96b2-2bd316023044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of images by extracting images from random indices sampled from the test data.\n",
    "batch = [test_dataset[i][\"image\"] for i in image_indices]\n",
    "batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6d33e48",
   "metadata": {},
   "source": [
    "#### Run batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9f0f1-be22-4910-a5a1-e6d861cf3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_maps = predict(\n",
    "    model=segformer,\n",
    "    feature_extractor=segformer_feature_extractor,\n",
    "    images=batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b7394d-eef4-498e-bee4-38214df85406",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_maps[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e378ad0",
   "metadata": {},
   "source": [
    "Performing batch inference outputs a list of segmentation maps. Each element in the segmentation map array represents the [semantic category](https://docs.google.com/spreadsheets/d/1se8YEtb2detS7OuPE86fXGyD269pMycAWe2mtKUj2W8/edit#gid=0) of the corresponding pixel in the input image.\n",
    "\n",
    "Together, you can visualize these predicted segmentation maps by overlaying them onto the original image to see defined regions of objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee752e3a-d615-4773-a932-83018aee119e",
   "metadata": {},
   "source": [
    "#### Visualize example predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d6a72-ed3e-4548-9b04-8490ee25f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualize_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ef21c-879e-46b3-b97f-1c00efcae05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(image=batch[0], segmentation_maps=segmentation_maps[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52be8531-ad94-4fde-b13d-a864f3020a37",
   "metadata": {},
   "source": [
    "### Run sequential inference on 10 batches\n",
    "\n",
    "Next, you will test the scalability and performance of the sequential batch inference approach by increasing the number of batches from 1 to 10. This will allow you to observe and verify that this approach can limit performance when scaling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93faafea-ddec-4148-a60a-a89d22b12aa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Prepare batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e17bf3-9c04-47eb-a7c5-604a0ffbb3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "N_BATCHES = 10\n",
    "\n",
    "# Get BATCH_SIZE * N_BATCHES randomly shuffled image IDs from the test dataset.\n",
    "image_indices = get_image_indices(dataset=test_dataset, n=BATCH_SIZE * N_BATCHES)\n",
    "\n",
    "# Split indices into N_BATCHES\n",
    "image_indices_grouped = np.split(np.asarray(image_indices), N_BATCHES)\n",
    "image_indices_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6004f-1cd5-46c9-89ad-708bbfe67312",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "\n",
    "# Create a list of images for each batch of indices sampled from the test dataset.\n",
    "for image_idx in image_indices_grouped:\n",
    "    batch = [test_dataset[int(i)][\"image\"] for i in image_idx]\n",
    "    batches.append(batch)\n",
    "\n",
    "batches[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7907202d-bc99-40d0-aa15-e8200a206426",
   "metadata": {},
   "source": [
    "#### Run batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca1763-8d9d-46aa-90c0-29b4b2be3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154c7c3-8fae-44f9-9e4c-bca3b62b7328",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batches:\n",
    "    segmentation_maps = predict(\n",
    "        model=segformer,\n",
    "        feature_extractor=segformer_feature_extractor,\n",
    "        images=batch,\n",
    "    )\n",
    "    predictions.append(segmentation_maps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d24d6603-50ee-4ed7-b17e-411fb5ca92b1",
   "metadata": {},
   "source": [
    "Notice that increasing the number of batches by 10 leads to approximately a 10x increase in runtime/ This is the expected result for a sequential approach, which scales linearly with the number of batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d425f7-d23c-41d1-adee-3cc9509e190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the resulting segmentation maps array.\n",
    "predictions[0][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21e69cad-d947-42d0-9cad-8f0a2d4941c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary: Sequential batch inference\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/single_sequential_timeline.png\" width=\"90%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Timeline of sequential batch inference using a single worker. Tasks can vary in runtime due variations in complexity, data size, and more. |\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>Batch inference</strong> (also known as offline inference): is the process of generating predictions on a large set or \"batch\" of data.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cba2f68-c762-439e-850b-3af83ba1eaae",
   "metadata": {},
   "source": [
    "## Part 4: Distributed, stateless batch inference with Ray Tasks\n",
    "\n",
    "Now that you have learned how to implement batch inference sequentially for this semantic segmentation task, it's time to explore three different distributed batch inference approaches using Ray. \n",
    "\n",
    "Starting with stateless inference with Ray Tasks, you will load the replicas of the SegFormer model and feature extractor onto tasks which will run inference on different batches concurrently. Because these tasks do not store or modify any internal state, we refer to them as stateless tasks.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/task_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|In the stateless inference approach, each Ray Task loads the trained model and generates predictions for the assigned batches. This method scales well with the number of available CPUs and GPUs because each inference task can be executed concurrently and independently of the other tasks.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665bc8b6-878e-4fde-9054-2891acf83c1b",
   "metadata": {},
   "source": [
    "### Initialize Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce928af7-2037-4b65-9dd1-6d67b5335e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c97168",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc24d09d-3b63-4307-a47e-1a72b2e33c9a",
   "metadata": {},
   "source": [
    "### Put the model and feature extractor in the object store\n",
    "\n",
    "When using Ray, you can pass objects as arguments to remote functions. Ray will automatically store these objects in the local object store (on the worker node where the function is running) using the [`ray.put()`](https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-put) function. This makes the objects available to all local tasks. However, if the objects are large, this can be inefficient as the objects will need to be copied every time they are passed to a remote function.\n",
    "\n",
    "To improve performance, you can explicitly store both the model and feature extractor in the object store by using `ray.put()`. This avoids the need to create multiple copies of the objects.\n",
    "\n",
    "It is important to note that if you have multiple worker nodes in your cluster, the objects will need to be copied in memory when they are used on a worker node different from where they are stored. Zero copy is not guaranteed in this case.\n",
    "\n",
    "|<img src=\"../../_static/assets/Overview_of_Ray/object_store.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Diagram of workers in worker nodes using `ray.put()` to place objects and using `ray.get()` to retrieve them from each node's object store.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abbe8d8-f00d-4ba8-a81f-aa0a5f480854",
   "metadata": {},
   "outputs": [],
   "source": [
    "segformer_ref = ray.put(segformer)\n",
    "segformer_feature_extractor_ref = ray.put(segformer_feature_extractor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ffc8a08-5629-42ae-94a0-1bdafad47c74",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>Tip</strong>\n",
    "\n",
    "  Passing the same large argument (model), by value repeatedly <a href=\"https://docs.ray.io/en/latest/ray-core/patterns/pass-large-arg-by-value.html\">harms performance and can cause Out-of-disk for the driver node</a>.\n",
    "  \n",
    "  Use the **object store** and **ray.put()** to pass by reference instead (for example, model_ref instead of model).\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "946db436-ac8b-4ac9-8dca-e557dbd126fb",
   "metadata": {},
   "source": [
    "### Define remote function for inference\n",
    "\n",
    "One way to parallelize predictions in a stateless manner (similar to using lambdas) is to use Ray tasks. Each time a Ray task is called, it loads the trained model from the local object store in order to perform inference. \n",
    "\n",
    "This approach allows the prediction task to be stateless, but it incurs the overhead of loading the model each time it is called. This may not be a significant issue for small models, but larger models may experience bottlenecks when loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b9e38",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Use decorator to designate this as a remote function.\n",
    "@ray.remote\n",
    "def inference_task(\n",
    "    model: SegformerForSemanticSegmentation,\n",
    "    feature_extractor: SegformerFeatureExtractor,\n",
    "    images: list[JpegImageFile],\n",
    ") -> list[np.array]:\n",
    "    # The `predict` function is the same one defined earlier in Part 3.\n",
    "    return predict(\n",
    "        model=model,\n",
    "        feature_extractor=feature_extractor,\n",
    "        images=images,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e695c89f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>Tip</strong>: Batches should be large enough to avoid the anti-pattern of having  <a href=\"https://docs.ray.io/en/latest/ray-core/patterns/too-fine-grained-tasks.html\"> tasks which are too fine-grained</a>.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c403178-5b95-4849-bb9d-fe453c88f0f5",
   "metadata": {},
   "source": [
    "### Run parallel inference on 10 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c9494-40f9-4ddc-904e-f1e12a5e0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_refs = []\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc90802-72b2-4ac1-b61d-60b32cdbad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch all prediction tasks.\n",
    "for batch in batches:\n",
    "    # Launch a prediction task by passing model reference, feature extractor\n",
    "    # reference, and batch of images.\n",
    "    task_ref = inference_task.remote(\n",
    "        model=segformer_ref,\n",
    "        feature_extractor=segformer_feature_extractor_ref,\n",
    "        images=batch,\n",
    "    )\n",
    "    # Collect all object references to batches.\n",
    "    prediction_refs.append(task_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5366e165-3aec-4e91-9693-53bbef29c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve results.\n",
    "# Note: This is a synchronous/blocking operation which waits for all processes to complete\n",
    "# before returning.\n",
    "predictions = ray.get(prediction_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d4d685-315f-4bc9-8d25-e3422e12054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the resulting segmentation maps array.\n",
    "predictions[0][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fadeb4b",
   "metadata": {},
   "source": [
    "**Coding Exercise**\n",
    "\n",
    "You have seen how the sequential version and stateless inference using Ray Tasks performs on 10 batches of 16 images each. Try scaling the number of batches as well as the number of images per batch to see the effect on performance.\n",
    "\n",
    "Hint: `BATCH_SIZE` and `N_BATCHES` is set in the Part 3 under \"Prepare batches\"\n",
    "\n",
    "Note: In order to perform inference on more than 160 images, you need to set the `SMALL_DATA` flag to `False` to download the complete test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "711cf20f-4cb2-4e16-ad66-bd273352c9d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary: Distributed, stateless batch inference with Ray Tasks\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>Object store</strong>: Ray's distributed shared-memory store that makes remote objects available anywhere in a Ray cluster.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>Stateless inference</strong>: Inference that depends only on an inputted trained model and does not preserve state once predictions are generated.\n",
    "</div>\n",
    "\n",
    "#### Key API elements\n",
    "\n",
    "* **`ray.init()`**  \n",
    "Start Ray runtime and connect to the Ray cluster.\n",
    "\n",
    "* **`@ray.remote`**  \n",
    "Decorator that specifies a Python function or class to be executed as a task (remote function) or actor (remote class) in a different process.\n",
    "\n",
    "* **`.remote`**  \n",
    "Postfix to the remote functions and classes; remote operations are asynchronous.\n",
    "\n",
    "* **`ray.put()`**  \n",
    "Put an object in the in-memory object store; returns an object reference used to pass the object to any remote function or method call.\n",
    "\n",
    "* **`ray.get()`**  \n",
    "Get a remote object(s) from the object store by specifying the object reference(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e2f9e2-945b-4b08-95ec-74b09b8b07d5",
   "metadata": {},
   "source": [
    "## Part 5: Distributed, stateful batch inference with Ray Actors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ff1bb7d-b8f9-4a2d-89d2-01446e114525",
   "metadata": {},
   "source": [
    "Moving from stateless to stateful inference, Ray Actors offer the advantage of holding some mutable internal state which allows the actor to fetch the model once and reuse it for all tasks assigned to the actor.\n",
    "\n",
    "To set up stateful inference using Ray Actors, you will need to follow a few important steps:\n",
    "\n",
    "1. Create replicas of your trained model as Ray Actors, which can hold mutable internal state and avoid the need to reload large models for each inference job.\n",
    "2. Feed data into these model replicas in parallel and retrieve predictions.\n",
    "\n",
    "You can either manually assign tasks to actors (more control) or use the ActorPool utility (more convenient), which automates load balancing for you. If you choose to assign actors manually, you will need to continually manage idle actors and assign tasks until the entire inference job is completed.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/actor_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray Actors can generate predictions on batches of data. Because each actor keeps track of an internal state, it can be reused for inference on multiple batches.|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70563f89-c95c-4ee8-8374-0288d918d5fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define remote class for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030c8a1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Specify this class as a Ray Actor.\n",
    "@ray.remote\n",
    "class PredictionActor:\n",
    "\n",
    "    # An interface (i.e. constructor) to load/cache the model and feature extractor.\n",
    "    def __init__(self, model, feature_extractor):\n",
    "        self.model = model\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    # This is the same logic as the `predict()` function defined in Part 3.\n",
    "    def predict(self, images):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        inputs = self.feature_extractor(images=images, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(pixel_values=inputs.pixel_values.to(device))\n",
    "\n",
    "        image_sizes = [image.size[::-1] for image in images]\n",
    "        segmentation_maps_postprocessed = (\n",
    "            self.feature_extractor.post_process_semantic_segmentation(\n",
    "                outputs=outputs, target_sizes=image_sizes\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return [j.detach().cpu().numpy() for j in segmentation_maps_postprocessed]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd911b90-2006-4fc9-bbbb-90e92ef79ee3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a list of Ray Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f910cd41",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "N_ACTORS = 2\n",
    "\n",
    "# Create a list of actors with N_ACTORS instances of `PredictionActor`.\n",
    "actors = [\n",
    "    PredictionActor.remote(\n",
    "        model=segformer_ref, feature_extractor=segformer_feature_extractor_ref\n",
    "    )\n",
    "    for _ in range(N_ACTORS)\n",
    "]\n",
    "\n",
    "actors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b915bfba-7baf-4761-a99d-e6e36dfbfa0f",
   "metadata": {},
   "source": [
    "Note: `N_ACTORS` is initally set to 2 here, which hinders performance. Ideally, you want to set the number of actors to be proportional to the amount of resources you have available, such as number of CPUs and/or GPUs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93d39dc3-a2ac-45a6-aab6-6a26147dd9b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run parallel inference on 10 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a80f96-3561-4435-8661-1b989c22692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_results_postprocessing(\n",
    "    predictions: list[list[np.array]], segmentation_maps: list[np.array]\n",
    ") -> list[list[np.array]]:\n",
    "    predictions.append(segmentation_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d89d1",
   "metadata": {},
   "source": [
    "`prediction_results_postprocessing` is simple function in this tutorial and exists to abstract away the final processing step. In practice it will likely be much more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa802a4b-24cc-4716-bb94-a033b0394b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []  # A list of final predictions.\n",
    "future_to_actor_mapping = (\n",
    "    {}\n",
    ")  # A mapping of object references to the actor that promised them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbce1966-c4be-406e-8df1-4ff1650b531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy to avoid modifying the original list of actors.\n",
    "idle_actors = actors.copy()\n",
    "\n",
    "while batches:\n",
    "\n",
    "    # Assign batches to available actors.\n",
    "    if idle_actors:\n",
    "        actor = idle_actors.pop()\n",
    "        batch = batches.pop()\n",
    "        future = actor.predict.remote(images=batch)\n",
    "        # Map the future to the actors executing prediction.\n",
    "        future_to_actor_mapping[future] = actor\n",
    "\n",
    "    # Retrieve the completed tasks and process them.\n",
    "    else:\n",
    "        # Retrieve the first future to return.\n",
    "        [ready], _ = ray.wait(list(future_to_actor_mapping.keys()), num_returns=1)\n",
    "\n",
    "        # Get the actor with the completed task and add back to idle list.\n",
    "        actor = future_to_actor_mapping.pop(ready)\n",
    "        idle_actors.append(actor)\n",
    "\n",
    "        # Post-processing on on result using ray.get() to retrieve result from reference.\n",
    "        prediction_results_postprocessing(\n",
    "            predictions=predictions, segmentation_maps=ray.get(ready)\n",
    "        )\n",
    "\n",
    "# Process any leftover results at the end.\n",
    "for future in future_to_actor_mapping.keys():\n",
    "    prediction_results_postprocessing(\n",
    "        predictions=predictions, segmentation_maps=ray.get(future)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "462f5b7b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>Tip</strong>: <a href=\"https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-wait\">ray.wait() </a> is a synchronous operation that allows you to process results without waiting on all tasks to complete. It also <a href=\"https://docs.ray.io/en/master/ray-core/patterns/limit-pending-tasks.html\"> limits the number of  pending tasks </a> so that the pending task queue won't grow indefinitely and cause out of memory problems.\n",
    "</div>\n",
    "\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/distributed_timeline.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Timeline of distributed batch inference where batches are assigned as soon as a worker completes a task and becomes available.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a7b0fd-b23e-4212-bfcd-483c6a604376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the resulting segmentation maps array.\n",
    "predictions[0][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c60cb444",
   "metadata": {},
   "source": [
    "**Coding Exercise**\n",
    "\n",
    "In this tutorial, the default setting for `N_ACTORS` is 2. Try setting the number of actors to the number of CPUs/GPUs you have available. \n",
    "\n",
    "How does this affect runtime performance?\n",
    "\n",
    "Hint: Change `N_ACTORS` in the section called \"Create list of Ray Actors.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7036efa3-2dd1-4b55-82e3-da81a0a33f13",
   "metadata": {},
   "source": [
    "### Using Ray ActorPool utility\n",
    "\n",
    "You have just manually managed batch assignment and task scheduling on Ray Actors for batch inference. This offers plenty of granular control over exactly how to distribute work and monitor in-flight tasks. \n",
    "\n",
    "However, you may choose to opt to use the convenient Ray [ActorPool](https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-util-actorpool) utility which wraps the list of actors to automatically handle futures management. In this short example, we will recreate this approach and demonstrate how to use this utility abstraction.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/actor_pool.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|The ActorPool wraps around a list of `n` actors so you do not have to manage idle actors and manually distribute workloads.|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b35ca662-e250-449d-9e20-cd233d61452c",
   "metadata": {},
   "source": [
    "### Prepare batches\n",
    "\n",
    "Recreate the batches for inference because the last batches were popped from the existing list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e7ee7e-d79b-48b3-b2a3-b387e738a745",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "N_BATCHES = 10\n",
    "\n",
    "# Get BATCH_SIZE * N_BATCHES randomly shuffled image IDs from the test dataset.\n",
    "image_indices = get_image_indices(dataset=test_dataset, n=BATCH_SIZE * N_BATCHES)\n",
    "\n",
    "# Split indices into N_BATCHES\n",
    "image_indices_grouped = np.split(np.asarray(image_indices), N_BATCHES)\n",
    "image_indices_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596c3661-3970-49f0-8cac-4f0c41164d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "\n",
    "# Create a list of images for each batch of indices sampled from the test dataset.\n",
    "for image_idx in image_indices_grouped:\n",
    "    batch = [test_dataset[int(i)][\"image\"] for i in image_idx]\n",
    "    batches.append(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a75b0-648e-4806-9ae6-f49f9ce10ba0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create ActorPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2baad5-ea29-44d3-88d8-28f81190caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.util.actor_pool import ActorPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97c5e3-0cec-41b2-ad44-0a5e4dda0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the actors in an ActorPool utility to automatically handle future management.\n",
    "actor_pool = ActorPool(actors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55c8ade5",
   "metadata": {},
   "source": [
    "Note: The [ActorPool](https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-util-actorpool) is fixed in size, unlike task-based approach where the number of parallel tasks can be dynamic. To have autoscaling of the ActorPool, you will need to use the Ray AIR approach discussed in the next approach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "755b1725-82ce-409f-8251-6eb24964fabc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run parallel inference on 10 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e7b08",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Runs prediction and returns object references to segmentation maps.\n",
    "def actor_call(\n",
    "    actor: ray.actor.ActorHandle, batch_of_images: list[list[JpegImageFile]]\n",
    ") -> list[ray._raylet.ObjectRef]:\n",
    "    return actor.predict.remote(images=batch_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058dbfae-10b2-452e-9054-e01062ca4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []  # A list of final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaacec5d-1e99-40bc-b160-642c2a32b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "for segmentation_maps in actor_pool.map_unordered(actor_call, batches):\n",
    "    prediction_results_postprocessing(\n",
    "        predictions=predictions, segmentation_maps=segmentation_maps\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "029149ee",
   "metadata": {},
   "source": [
    "By using the `ActorPool` utility, you were able to easily run distributed batch inference with just a few lines of code. The `map_unordered` function runs the defined inference logic on each batch and handles post-processing, eliminating the need for manual orchestration of actors. This simplifies the process and reduces the need for monitoring tasks and actors at various stages of completion.\n",
    "\n",
    "Note: `map_unordered` has slightly better efficiency that a similar method `actor_pool.map` since this example does not preference the order of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730b12c-c5e2-4935-8b06-d0d01b36e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the resulting segmentation maps array.\n",
    "predictions[0][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abfbe82e",
   "metadata": {},
   "source": [
    "**Coding Exercise**\n",
    "\n",
    "While the `ActorPool` utility offers a good level of abstraction above orchestrating actors directly, there are [methods](https://docs.ray.io/en/latest/ray-core/package-ref.html?highlight=actorpool#ray-util-actorpool) available to you to schedule tasks, inspect in-flight jobs, and retrieve idle actors.\n",
    "\n",
    "Try look into the actor pool by printing out which actors are idle and which tasks remain during the inferencing step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e215c15a-83ee-45d0-a8f7-53ef97cd0d28",
   "metadata": {},
   "source": [
    "#### Optional: Terminate actors after the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827dc61-4aed-49f5-80d4-926805cbe444",
   "metadata": {},
   "outputs": [],
   "source": [
    "if actor_pool.has_next() == False:\n",
    "    while actor_pool.has_free():\n",
    "        actor = actor_pool.pop_idle()\n",
    "        actor.__ray_terminate__.remote()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "941f3d6d-42c8-4726-ab46-21527fd737e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary: stateful inference with Ray Actors and ActorPool utility\n",
    "\n",
    "#### Key API elements\n",
    "\n",
    "* **`ActorPool()`**  \n",
    "Wraps the list of actors that run inference.\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>Stateful inference</strong>: Inference carried out over stateful processes where Ray actors hold model replicas and can mutate and persist state.\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7289f04c-337c-47e1-a415-a5c5d80b61cc",
   "metadata": {},
   "source": [
    "## Part 6: Distributed batch inference with Ray AIR\n",
    "\n",
    "Moving towards the higher-level APIs offered by Ray AIR, you'll see how to use [Ray Datasets](https://docs.ray.io/en/latest/data/dataset.html) to perform pre-processing and batching of data in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68550c41-7755-4863-900a-d8220d9d9b55",
   "metadata": {},
   "source": [
    "|<img src=\"../../_static/assets/Scaling_inference/ray_datasets.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray Datasets replace the 'Batch preprocessing' stage.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050e876-1b42-41ee-b3f2-d0caa8a31fbc",
   "metadata": {},
   "source": [
    "### Create Ray dataset with 160 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ccdeb8-4c31-4752-8f16-9f13e3cf3366",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "N_BATCHES = 10\n",
    "\n",
    "image_indices = get_image_indices(dataset=test_dataset, n=BATCH_SIZE * N_BATCHES)\n",
    "data = [test_dataset[i][\"image\"] for i in image_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968ed9fd",
   "metadata": {},
   "source": [
    "Once again, you prepare data batches by retrieving a random `BATCH_SIZE` number of images for every `N_BATCHES` and store them in the `data` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f4066-7d32-4cb6-813f-9a880263b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ray.data.from_items(data)\n",
    "dataset.show(limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2cf451",
   "metadata": {},
   "source": [
    "Then, you create a Ray Dataset from the list of data, and you can inspect that each item is a PIL image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4ba67-72f8-405c-b9c9-faccbe7900d1",
   "metadata": {},
   "source": [
    "### Implement class that computes predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54efdaa4-d0d6-473d-b647-0d7ee9a3e6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionClass:\n",
    "    def __init__(self, model, feature_extractor):\n",
    "        self.model = model\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        inputs = self.feature_extractor(images=batch, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(pixel_values=inputs.pixel_values.to(device))\n",
    "\n",
    "        image_sizes = [image.size[::-1] for image in batch]\n",
    "        segmentation_maps_postprocessed = (\n",
    "            self.feature_extractor.post_process_semantic_segmentation(\n",
    "                outputs=outputs, target_sizes=image_sizes\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return [j.detach().cpu().numpy() for j in segmentation_maps_postprocessed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147a110-10f8-4ee1-ab83-48116ac0e4e3",
   "metadata": {},
   "source": [
    "Each [instance](https://docs.ray.io/en/latest/data/transforming-datasets.html#transform-datasets-writing-udfs) of `PredictionClass` contains a replica of the model and feature extractor.\n",
    "\n",
    "Define the `__call__` method of the class to make it a callable class and specify the target method. The core logic of `__call__` remains the same as previous `predict()` functions.\n",
    "\n",
    "Given a `batch` (list), the [return type](https://docs.ray.io/en/latest/data/transforming-datasets.html#batch-udf-output-types) must be one of:\n",
    "\n",
    "* `pandas.DataFrame`\n",
    "* `pyarrow.Table`\n",
    "* `numpy.ndarray`\n",
    "* `Dict[str, numpy.ndarray]`\n",
    "* `list`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d526b-8d86-4316-9279-9f8ada88e863",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run parallel batch inference on 160 images and assess scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58424a37-642a-4906-86d4-f1e7b5a31058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data import ActorPoolStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744df868",
   "metadata": {},
   "source": [
    "In Ray Datasets, transformations can either be carried out by Ray Tasks or Actors. With `ActorPoolStrategy`, you can specify an [autoscaling](https://docs.ray.io/en/latest/data/transforming-datasets.html#compute-strategy) pool of `min` to `max` actors to carry out the transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daec098",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "predictions_dataset = dataset.map_batches(\n",
    "    PredictionClass,\n",
    "    batch_size=1,\n",
    "    num_gpus=0,\n",
    "    num_cpus=1,\n",
    "    compute=ActorPoolStrategy(min_size=1, max_size=2),\n",
    "    fn_constructor_args=(segformer, segformer_feature_extractor),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2824b4f4-94f3-4ac5-a06a-9bff6fed4086",
   "metadata": {},
   "source": [
    "Use the Dataset `map_batches()` [function](https://docs.ray.io/en/latest/data/api/dataset.html#ray.data.Dataset.map_batches) to apply the model to the Dataset in parallel. You can specify the batch size, any resources, as well as any autoscaling options for the actor pool.\n",
    "\n",
    "Note: don't forget to pass `fn_constructor_args` to construct `PredictionClass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac047c-8885-4c09-abae-99d2023c78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dataset.take(limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c1abb",
   "metadata": {},
   "source": [
    "**Coding Exercise**\n",
    "\n",
    "In this approach, you are able to control the actors using an `ActorPoolStrategy` which sets an upper and lower limit on the dynamic autoscaling of the pool. Try toggling the `min_size` and `max_size` of the actor pool in the inference step to see the effect on runtime performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f181c6e2",
   "metadata": {},
   "source": [
    "After running inference, you can inspect predictions to probe the resulting image array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c05d8-7b3b-4524-9f03-7ec64166bdd4",
   "metadata": {},
   "source": [
    "### Summary: batch inference with Ray AIR Datasets\n",
    "\n",
    "#### Key concepts\n",
    "* parallel reading and [preprocessing](https://docs.ray.io/en/master/data/transforming-datasets.html) of the source data\n",
    "* managing the autoscaling of the ActorPool using `ActorPoolStrategy`\n",
    "* declarative key-value arguments over fine-grain control over Ray\n",
    "\n",
    "#### Key API elements\n",
    "* `map_batches` - a function to apply a transformation and/or model class to all batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302f394-a33b-45e2-b527-6c74aea58f90",
   "metadata": {},
   "source": [
    "|<img src=\"../../_static/assets/Scaling_inference/air_batchpredictor.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Using Ray AIR's `BatchPredictor` for Batch Inference.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ef6e96-f29e-4db9-be57-0aee104a34d9",
   "metadata": {},
   "source": [
    "### Implement Predictor for image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae3c1e-31f6-4388-86a8-10ba84860f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3024c0-f651-44f4-93f6-8964918b3bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSegmentationPredictor(Predictor):\n",
    "    def __init__(self, model, feature_extractor):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def _predict_pandas(self, batch):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        batch = [batch[\"value\"][0]]\n",
    "        inputs = self.feature_extractor(images=batch, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(pixel_values=inputs.pixel_values.to(device))\n",
    "\n",
    "        image_sizes = [image.size[::-1] for image in batch]\n",
    "        segmentation_maps_postprocessed = (\n",
    "            self.feature_extractor.post_process_semantic_segmentation(\n",
    "                outputs=outputs, target_sizes=image_sizes\n",
    "            )\n",
    "        )\n",
    "\n",
    "        df = pd.DataFrame(columns=[\"segmentation_maps\"])\n",
    "        df.loc[0, \"segmentation_maps\"] = segmentation_maps_postprocessed\n",
    "\n",
    "        return df\n",
    "\n",
    "    @classmethod\n",
    "    def from_checkpoint(self, checkpoint, **kwargs):\n",
    "        checkpoint_data = checkpoint.to_dict()\n",
    "        return SemanticSegmentationPredictor(\n",
    "            model=checkpoint_data[\"model\"],\n",
    "            feature_extractor=checkpoint_data[\"feature_extractor\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09cd7a2-3825-49c4-a4f6-74ba71859ad1",
   "metadata": {},
   "source": [
    "Before you run inference, define a custom [Predictor](https://docs.ray.io/en/latest/ray-air/package-ref.html#predictor), `SemanticSegmentationPredictor`, with the same replicas and core `predict()` logic as before, with a few tweaks to fit this pattern.\n",
    "\n",
    "BatchPredictor also supports multiple framework specific predictors such as TorchPredictor and TensorflowPredictor along with providing support for framework native batch conversions, the ability to resume from an AIR checkpoint, keeping columns, and aggregating batch metrics.\n",
    "\n",
    "Note: batch in `_predict_pandas` is DataFrame rather than a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0790eeb-3207-4f8b-b520-e3751f93cf2b",
   "metadata": {},
   "source": [
    "### Implement BatchPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b73b9a-b04b-4ded-8322-fbc4fa2620a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air import Checkpoint\n",
    "from ray.train.batch_predictor import BatchPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3118f64",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "batch_predictor = BatchPredictor(\n",
    "    checkpoint=Checkpoint.from_dict(\n",
    "        {\"model\": segformer, \"feature_extractor\": segformer_feature_extractor}\n",
    "    ),\n",
    "    predictor_cls=SemanticSegmentationPredictor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce3a7c8",
   "metadata": {},
   "source": [
    "[`BatchPredictor`](https://docs.ray.io/en/latest/ray-air/predictors.html#batch-prediction) takes a [`Checkpoint`](https://docs.ray.io/en/latest/ray-air/package-ref.html#checkpoint) representing the saved model, and allows you to perform inference on an input dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0509bf8-d124-4a6c-9720-82bf01b03faa",
   "metadata": {},
   "source": [
    "### Run parallel batch inference on 160 images and assess scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74717154-8664-46b6-b54e-ec6d687d644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dataset = batch_predictor.predict(data=dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13caadb",
   "metadata": {},
   "source": [
    "Perform batch inference by using the simple API `batch_predictor.predict()` without specifying *how* execution should occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01247c69-c2f4-4563-a8d2-71b4e5b7b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dataset.take(limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e61745f",
   "metadata": {},
   "source": [
    "Once again, you can inspect the predictions to look at the resulting segmentation maps in this Pandas Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425505c4",
   "metadata": {},
   "source": [
    "**Coding Exercise**\n",
    "\n",
    "In our example, we used a custom `Predictor`, but Ray AIR's BatchPredictor offers support for a number of framework specific predictors. Referring to this [user guide] for assistance, try to implement the same inferencing logic, but this time, use a [HuggingFacePredictor](https://docs.ray.io/en/master/train/api.html?highlight=huggingfacepredictor#ray.train.huggingface.HuggingFacePredictor.predict) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc8a4e-a38e-4b7e-8c7c-d3fd6d2fc2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94188018-aa30-4860-9355-2b51bc4d0e00",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8a82cf-6e7d-48be-b708-6b6267401d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "import tempfile\n",
    "from ray.train.huggingface import HuggingFaceCheckpoint, HuggingFacePredictor\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    huggingface_checkpoint = HuggingFaceCheckpoint.from_model(\n",
    "        model=segformer, path=tmpdir\n",
    "    )\n",
    "    predictor = BatchPredictor.from_checkpoint(\n",
    "        checkpoint=huggingface_checkpoint,\n",
    "        predictor_cls=HuggingFacePredictor,\n",
    "        feature_extractor=segformer_feature_extractor,  # passed to HF pipeline\n",
    "        task=\"image-segmentation\",  # passed to HF pipeline\n",
    "        device=-1,\n",
    "    )\n",
    "\n",
    "predictions_dataset = predictor.predict(data=dataset, batch_size=1)\n",
    "predictions_dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a0b7a-3cff-4aea-9b92-c28200560728",
   "metadata": {},
   "source": [
    "### Shutdown Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3aae2c-491f-41a9-bd67-09395f398890",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cdc4c3-5c2e-489a-9382-d8ca46a26a1f",
   "metadata": {},
   "source": [
    "Disconnect the worker, and terminate processes started by `ray.init()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1a41b-0e05-427f-8cf2-ec393484bf16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary: BatchPredictor\n",
    "\n",
    "#### Key API elements\n",
    "\n",
    "* `BatchPredictor` - takes a predictor class and checkpoint and provides an interface to run batch scoring on Ray datasets; this batch predictor wraps around a predictor class and executes it in a distributed way when calling `predict()`\n",
    "* `Checkpoint` - a common interface for accessing models across different AIR components and libraries\n",
    "* `predict()` - run batch scoring on Dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d86ade2c-5e33-46af-804a-a10dd0e2bfc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 7: Architectures for scalable batch inference with Ray - final summary\n",
    "\n",
    "You learned about scalable batch inference patterns. Which one to choose depends on your use cases and needs. Here, you can find analysis that will help you make decisions.\n",
    "\n",
    "Overall, all patterns fall into intro 2 categories:\n",
    "\n",
    "1. **Ray core-based solutions** with task and actors (optionally with [ActorPool](https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-util-actorpool) utility) are suitable for users who want to specify how Ray should execute batch inference and have fine-grained control over application behavior.\n",
    "1. **Ray AIR-based solutions** with [Datasets](https://docs.ray.io/en/latest/data/api/dataset.html) and [BatchPredictor](https://docs.ray.io/en/latest/ray-air/package-ref.html#batch-predictor) allow you to define \"what\" instead of \"how\" and delegate responsibility for parallel execution to Ray.\n",
    "\n",
    "**Key differences**\n",
    "\n",
    "| |Ray Core|Ray AI Runtime|\n",
    "|:-:|:-:|:-:|\n",
    "|**Expose parallelism**|Yes|No|\n",
    "|**Scalable from workstation to large cluster**|Yes|Yes|\n",
    "|**Integrations**|Build yourself|Out-of-the-box (PyTorch, TF and [more](https://docs.ray.io/en/latest/ray-air/package-ref.html#trainer-and-predictor-integrations))|\n",
    "|**Data pre-processing**|Build yourself|Out-of-the-box support|\n",
    "\n",
    "**Developer experience**\n",
    "\n",
    "When working with [Ray core](https://docs.ray.io/en/latest/ray-core/walkthrough.html) and [Ray AIR](https://docs.ray.io/en/latest/ray-air/getting-started.html), you will notice that they have different focus. Ray core enables you to build and scale distributed Python applications. Ray AIR is a toolkit for simplifying ML compute.\n",
    "\n",
    "| |Ray Core|Ray AI Runtime|\n",
    "|:-:|:-:|:-:|\n",
    "|**Level of control**|High|Medium|\n",
    "|**Directly program distributed apps**|Yes|No|\n",
    "|**Flexibility**|High|Medium|\n",
    "|**Ease of use**|Medium|High|\n",
    "|**Entry barrier**|Depend on use case|Low|\n",
    "\n",
    "Ray core-based batch inference gives you exposed parallelism and high level of control, because you write Python code and scale it using Ray primitives. You control compute resources assigned to individual Ray tasks. In case of actors you also have access to the internal state of the distributed objects.\n",
    "\n",
    "With Ray AIR, parallel computing primitives (tasks, actors, object store) are abstracted away and handled internally. Because of that you do not need to worry about complex parallel computing logic and focus on your use case.\n",
    "\n",
    "**Recommendations***\n",
    "\n",
    "* Use Ray core-based solutions if you have batch inference pipeline with stages that are \"embarrassingly parallel\".\n",
    "* For most batch prediction problems **BatchPredictor is recommended**. Here is why:\n",
    "    * Ray Datasets handle distributed processing, creating batches of data and ensuring compute resources utilization.\n",
    "    * On top of that [BatchPredictor](https://docs.ray.io/en/latest/ray-air/package-ref.html#batch-predictor) gives you out-of-the-box predictors, handles framework native batch conversions and gives you an option to resume from AIR checkpoint.\n",
    "    * Ray AIR libraries are connected, giving you an option to scale other parts of your pipeline in the future.\n",
    "\n",
    "*Above-mentioned recommendations are meant to provide initial advice. They are not definitive guide, because final recommendations depend on your use case. Reach out on our Slack to discuss it deeper.*\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/actor_inference.png\" width=\"90%\" loading=\"lazy\">|<img src=\"../../_static/assets/Scaling_inference/air_batchpredictor.png\" width=\"90%\" loading=\"lazy\">|\n",
    "|:-:|:-:|\n",
    "|Ray core: actors|Ray AIR: batch prediction|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe90b042-e31b-483e-8368-5d7b7aa1c761",
   "metadata": {},
   "source": [
    "# Connect with the Ray community\n",
    "\n",
    "You can learn and get more involved with the Ray community of developers and researchers:\n",
    "\n",
    "* [**Ray documentation**](https://docs.ray.io/en/latest)\n",
    "\n",
    "* [**Official Ray Website**](https://www.ray.io/)  \n",
    "Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.\n",
    "\n",
    "* [**Join the Community on Slack**](https://forms.gle/9TSdDYUgxYs8SA9e8)  \n",
    "Find friends to discuss your new learnings in our Slack space.\n",
    "\n",
    "* [**Use the Discussion Board**](https://discuss.ray.io/)  \n",
    "Ask questions, follow topics, and view announcements on this community forum.\n",
    "\n",
    "* [**Join a Meetup Group**](https://www.meetup.com/Bay-Area-Ray-Meetup/)  \n",
    "Tune in on meet-ups to listen to compelling talks, get to know other users, and meet the team behind Ray.\n",
    "\n",
    "* [**Open an Issue**](https://github.com/ray-project/ray/issues/new/choose)  \n",
    "Ray is constantly evolving to improve developer experience. Submit feature requests, bug-reports, and get help via GitHub issues.\n",
    "\n",
    "* [**Become a Ray contributor**](https://docs.ray.io/en/latest/ray-contribute/getting-involved.html)  \n",
    "We welcome community contributions to improve our documentation and Ray framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891f0e7-56f7-41c5-b130-6bbee6e878f8",
   "metadata": {},
   "source": [
    "<img src=\"../../_static/assets/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "567405a8058597909526349386224fe35dd047505a91307e44ed44be00113429"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
